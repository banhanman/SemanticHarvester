import requests
from bs4 import BeautifulSoup
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
import argparse
import json
import re
import sys

try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('punkt')
    nltk.download('stopwords')

def get_page_content(url):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {str(e)}")
        return None

def extract_text(html):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML —Å –æ—á–∏—Å—Ç–∫–æ–π"""
    soup = BeautifulSoup(html, 'lxml')
    
    for element in soup(["script", "style", "meta", "link", "footer", "nav", "header"]):
        element.decompose()
    
    text = soup.get_text()
    lines = (line.strip() for line in text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    return '\n'.join(chunk for chunk in chunks if chunk)

def analyze_keywords(text, lang='english', top_n=20):
    """–ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NLP"""
    words = word_tokenize(text.lower())
    words = [word for word in words if word.isalnum() and len(word) > 2]
    
    stop_words = set(stopwords.words(lang))
    filtered_words = [word for word in words if word not in stop_words]
    
    word_freq = Counter(filtered_words)
    return word_freq.most_common(top_n)

def detect_language(text):
    """–ü—Ä–æ—Å—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –ø–æ –ø—Ä–µ–æ–±–ª–∞–¥–∞—é—â–∏–º —Å–∏–º–≤–æ–ª–∞–º"""
    if re.search(r'[–∞-—è—ë]', text, re.IGNORECASE):
        return 'russian'
    return 'english'

def process_site(url):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç–∞"""
    print(f"\n[üîç] –ê–Ω–∞–ª–∏–∑: {url}")
    html = get_page_content(url)
    if not html:
        return None
    
    text = extract_text(html)
    if not text.strip():
        print(f"[‚ö†] –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ {url}")
        return None
    
    lang = detect_language(text)
    keywords = analyze_keywords(text, lang)
    
    return {
        'url': url,
        'language': lang,
        'keywords': keywords,
        'text_sample': text[:500] + "..."  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–∑–µ—Ü —Ç–µ–∫—Å—Ç–∞
    }

def save_results(results, output_file):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\n[üíæ] –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")

def main():
    parser = argparse.ArgumentParser(description='SemanticHarvester - –°–±–æ—Ä —Å–µ–º–∞–Ω—Ç–∏–∫–∏ —Å–∞–π—Ç–æ–≤')
    parser.add_argument('sites', nargs='+', help='URL —Å–∞–π—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞')
    parser.add_argument('-o', '--output', default='semantic_results.json', help='–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: semantic_results.json)')
    
    args = parser.parse_args()
    
    if not args.sites:
        print("–û—à–∏–±–∫–∞: –ù–µ —É–∫–∞–∑–∞–Ω—ã URL —Å–∞–π—Ç–æ–≤")
        sys.exit(1)
    
    results = []
    for url in args.sites:
        result = process_site(url)
        if result:
            results.append(result)
            print(f"[‚úÖ] –ù–∞–π–¥–µ–Ω–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {len(result['keywords'])}")
            print("    –¢–æ–ø-5 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤:")
            for word, freq in result['keywords'][:5]:
                print(f"    - {word}: {freq}")
    
    if results:
        save_results(results, args.output)
    else:
        print("[‚ùå] –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ —Å –æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç–∞")

if __name__ == "__main__":
    main()
